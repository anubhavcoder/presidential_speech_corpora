{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: \n",
    "* Student pace: self paced / part time / full time\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: \n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import phrases\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "import os, string, re\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=1\n",
    "# selection_index = {}\n",
    "# for pres in sorted(list(os.listdir('Corpus of Presidential Speeches/')))[1:]:\n",
    "#     if (pres == 'hclinton') or (pres == 'trump'):\n",
    "#         pass\n",
    "#     else:\n",
    "#         print(f'{i}:\\t{pres.title()}')\n",
    "#         selection_index[i] = pres\n",
    "#         i += 1\n",
    "    \n",
    "# selections = [input('\\nEnter the number of the first president/candidate you would like to compare:')]\n",
    "# selections.append(input('Enter the number of the second president/candidate you would like to compare:'))\n",
    "# selections = [selection_index[int(selections[0])], selection_index[int(selections[1])]]\n",
    "# selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2206, 4)\n",
      "{0: 'hclinton', 1: 'trump'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[hello, philadelphia, oh, thank, you, i, am, s...</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[election, a, choice, between, division, or, u...</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[son, captain, khan, was, killed, serving, our...</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[president, obama, said, it, started, right, h...</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[my, opponent, accuses, me, of, playing, the, ...</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   president      name                                               text  \\\n",
       "0          0  Hclinton  [hello, philadelphia, oh, thank, you, i, am, s...   \n",
       "1          0  Hclinton  [election, a, choice, between, division, or, u...   \n",
       "2          0  Hclinton  [son, captain, khan, was, killed, serving, our...   \n",
       "3          0  Hclinton  [president, obama, said, it, started, right, h...   \n",
       "4          0  Hclinton  [my, opponent, accuses, me, of, playing, the, ...   \n",
       "\n",
       "   length  \n",
       "0     250  \n",
       "1     250  \n",
       "2     250  \n",
       "3     250  \n",
       "4     250  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selections = ['trump', 'hclinton']\n",
    "t0 = datetime.datetime.now()\n",
    "lvl1 = 'Corpus of Presidential Speeches/'\n",
    "\n",
    "speech_dict = {'president': [], 'name': [], 'text': []}\n",
    "labels_index = {}\n",
    "speech_count = {}\n",
    "\n",
    "i=0\n",
    "for folder in os.listdir(lvl1):\n",
    "    if folder in selections:\n",
    "        labels_index[i] = folder.lower()\n",
    "        speech_count[folder] = [0]\n",
    "        \n",
    "        for file in os.listdir(lvl1 + folder):\n",
    "            speech_count[folder][0] += 1\n",
    "            \n",
    "            with open(f'{lvl1}{folder}/{file}', 'r') as f:\n",
    "                data = f.read()\n",
    "                \n",
    "                # Remove tags for date, title, etc. & punctuation\n",
    "                no_tags = re.sub('<[^>]+>', '', data)\n",
    "                paragraphs = no_tags.translate(str.maketrans('', '', string.punctuation))\n",
    "                \n",
    "                # Turn data into list of lower-case words\n",
    "                words = paragraphs.split()\n",
    "                words = [x.strip().lower() for x in words] \n",
    "#                 words = [word.lower() for word in words if word]\n",
    "                \n",
    "                # Every 1000 words treated as paragraph; remove stop words\n",
    "                n = len(words) // 250\n",
    "                for j in range(n):\n",
    "                    text = words[j*250:(j+1)*250]\n",
    "                    speech_dict['text'].append([word for word in text])\n",
    "                    speech_dict['president'].append(i)\n",
    "                    speech_dict['name'].append(folder.title())\n",
    "                    \n",
    "                text = words[n*250:]\n",
    "                speech_dict['text'].append([word for word in text])\n",
    "                speech_dict['president'].append(i)\n",
    "                speech_dict['name'].append(folder.title())\n",
    "        i+=1\n",
    "                \n",
    "df = pd.DataFrame.from_dict(speech_dict)\n",
    "df['length'] = df['text'].apply(lambda x: len(x))\n",
    "df = df[df['length'] >= 25]\n",
    "num_classes = len(df['president'].unique())\n",
    "print(df.shape)\n",
    "print(labels_index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[hello, philadelphia, oh, thank, you, i, am, s...</td>\n",
       "      <td>250</td>\n",
       "      <td>[hello, philadelphia, oh, thank, grateful, ton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[election, a, choice, between, division, or, u...</td>\n",
       "      <td>250</td>\n",
       "      <td>[election, choice, division, unity, economy, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[son, captain, khan, was, killed, serving, our...</td>\n",
       "      <td>250</td>\n",
       "      <td>[son, captain, khan, killed, serving, country,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[president, obama, said, it, started, right, h...</td>\n",
       "      <td>250</td>\n",
       "      <td>[president, obama, said, started, right, phila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[my, opponent, accuses, me, of, playing, the, ...</td>\n",
       "      <td>250</td>\n",
       "      <td>[opponent, accuses, playing, womens, card, wel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   president      name                                               text  \\\n",
       "0          0  Hclinton  [hello, philadelphia, oh, thank, you, i, am, s...   \n",
       "1          0  Hclinton  [election, a, choice, between, division, or, u...   \n",
       "2          0  Hclinton  [son, captain, khan, was, killed, serving, our...   \n",
       "3          0  Hclinton  [president, obama, said, it, started, right, h...   \n",
       "4          0  Hclinton  [my, opponent, accuses, me, of, playing, the, ...   \n",
       "\n",
       "   length                                         clean_text  \n",
       "0     250  [hello, philadelphia, oh, thank, grateful, ton...  \n",
       "1     250  [election, choice, division, unity, economy, w...  \n",
       "2     250  [son, captain, khan, killed, serving, country,...  \n",
       "3     250  [president, obama, said, started, right, phila...  \n",
       "4     250  [opponent, accuses, playing, womens, card, wel...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "remove_stop = []\n",
    "for i in range(df.shape[0]):\n",
    "    words = df.iloc[i]['text']\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    remove_stop.append(words)\n",
    "    \n",
    "df['clean_text'] = remove_stop\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trump       1738\n",
       "Hclinton     468\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "sns.countplot('name', data=df)\n",
    "plt.title(f'Paragraph Count by {df.columns[0].title()}')\n",
    "plt.ylabel('Paragraph Count')\n",
    "plt.xlabel(df.columns[0].title())\n",
    "plt.savefig(f'{selections[0]}_{selections[1]}_paragraph_count.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shuffle the Dataset.\n",
    "shuffled_df = df.sample(frac=1,random_state=4)\n",
    "\n",
    "# Create dictionary to store scaled dataframes for each president.\n",
    "scaled_dfs = {}\n",
    "\n",
    "# Set max samples based on president with fewest paragraphs\n",
    "n = shuffled_df.loc[shuffled_df['name'] == df['name'].value_counts().index[-1]].shape[0]\n",
    "\n",
    "#Randomly select 492 observations from the non-fraud (majority class)\n",
    "for name in df['name'].unique():\n",
    "    scaled_dfs[f'{name}_df'] = shuffled_df.loc[shuffled_df['name'] == name].sample(n=n,random_state=42)\n",
    "\n",
    "# Concatenate dataframes\n",
    "scaled_dfs = [scaled_dfs[key] for key in list(scaled_dfs.keys())]\n",
    "normalized_df = pd.concat(scaled_dfs)\n",
    "\n",
    "#plot the dataset after the undersampling\n",
    "plt.figure(figsize=(25, 8))\n",
    "sns.countplot('name', data=normalized_df)\n",
    "plt.title(f'Balanced Paragraph Count by {df.columns[0].title()}')\n",
    "plt.ylabel('Paragraph Count')\n",
    "plt.xlabel(df.columns[0].title())\n",
    "plt.savefig(f'{selections[0]}_{selections[1]}_bal_paragraph_count.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(936, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[job, that, yes, pay, a, salary, but, doesnt, ...</td>\n",
       "      <td>250</td>\n",
       "      <td>[job, yes, pay, salary, doesnt, give, meaning,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[two, buildings, all, the, while, hes, been, r...</td>\n",
       "      <td>250</td>\n",
       "      <td>[two, buildings, hes, running, around, ohio, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[the, rising, tide, of, hardline, rightwing, n...</td>\n",
       "      <td>250</td>\n",
       "      <td>[rising, tide, hardline, rightwing, nationalis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[whoa, it, is, so, great, to, be, here, thank,...</td>\n",
       "      <td>250</td>\n",
       "      <td>[whoa, great, thank, much, backstage, listenin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>Hclinton</td>\n",
       "      <td>[hello, pittsburgh, whoa, hello, back, there, ...</td>\n",
       "      <td>250</td>\n",
       "      <td>[hello, pittsburgh, whoa, hello, back, thank, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     president      name                                               text  \\\n",
       "452          0  Hclinton  [job, that, yes, pay, a, salary, but, doesnt, ...   \n",
       "217          0  Hclinton  [two, buildings, all, the, while, hes, been, r...   \n",
       "247          0  Hclinton  [the, rising, tide, of, hardline, rightwing, n...   \n",
       "182          0  Hclinton  [whoa, it, is, so, great, to, be, here, thank,...   \n",
       "29           0  Hclinton  [hello, pittsburgh, whoa, hello, back, there, ...   \n",
       "\n",
       "     length                                         clean_text  \n",
       "452     250  [job, yes, pay, salary, doesnt, give, meaning,...  \n",
       "217     250  [two, buildings, hes, running, around, ohio, t...  \n",
       "247     250  [rising, tide, hardline, rightwing, nationalis...  \n",
       "182     250  [whoa, great, thank, much, backstage, listenin...  \n",
       "29      250  [hello, pittsburgh, whoa, hello, back, thank, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = normalized_df.president\n",
    "data = normalized_df['clean_text'].values\n",
    "\n",
    "max_len = 0\n",
    "for point in data:\n",
    "    max_len = len(point) if len(point) > max_len else max_len\n",
    "    \n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['job', 'yes', 'pay', 'salary', 'doesnt', 'give', 'meaning', 'purpose', 'lives', 'love', 'fact', 'service', 'come', 'new', 'ways', 'giving', 'back', 'find', 'path', 'charities', 'nonprofit', 'organizations', 'ways', 'serve', 'thats', 'sign', 'true', 'commitment', 'ive', 'also', 'impressed', 'many', 'young', 'entrepreneurs', 'tell', 'theyre', 'building', 'social', 'component', 'businesses', 'whether', 'dedicating', 'portion', 'profits', 'charity', 'giving', 'employees', 'time', 'volunteer', 'adopting', 'school', 'community', 'center', 'pro', 'bono', 'project', 'whole', 'company', 'thats', 'plan', 'help', 'refinance', 'pay', 'back', 'student', 'loans', 'includes', 'loan', 'forgiveness', 'launch', 'social', 'enterprise', 'start', 'business', 'underserved', 'community', 'make', 'easier', 'young', 'innovators', 'follow', 'dreams', 'honoring', 'values', 'giving', 'back', 'also', 'want', 'get', 'older', 'americans', 'involved', 'service', 'isnt', 'something', 'students', 'young', 'people', 'know', 'intend', 'make', 'sure', '10', 'percent', 'americorps', 'slots', 'go', 'americans', 'age', '55', 'lets', 'give', 'people', 'encore', 'opportunity', 'theyve', 'ended', 'formal', 'career', 'apply', 'lifetime', 'knowledge', 'experience', 'toward', 'stronger', 'community', 'finally', 'want', 'create', 'new', 'means', 'people', 'serve', 'serious', 'meaningful', 'ways', 'without', 'fulltime', 'commitment']\n"
     ]
    }
   ],
   "source": [
    "bigrams_data = data\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "job yes pay salary doesnt give meaning purpose lives love fact service come new ways giving back find path charities nonprofit organizations ways serve thats sign true commitment ive also impressed many young entrepreneurs tell theyre building social component businesses whether dedicating portion profits charity giving employees time volunteer adopting school community center pro bono project whole company thats plan help refinance pay back student loans includes loan forgiveness launch social enterprise start business underserved community make_easier young innovators follow dreams honoring values giving back also want get older americans involved service isnt something students young_people know intend make_sure 10_percent americorps slots go americans age 55 lets give people encore opportunity theyve ended formal career apply lifetime knowledge experience toward stronger community finally want create new means people serve serious meaningful ways without fulltime commitment\n"
     ]
    }
   ],
   "source": [
    "bigrams = phrases.Phrases(data)\n",
    "bigrams_data = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    bigrams_data.append(bigrams[data[i]])\n",
    "    \n",
    "bigrams_data = np.array(bigrams_data)\n",
    "\n",
    "max_len = 0\n",
    "for bigram in bigrams_data:\n",
    "    max_len = len(bigram) if len(bigram) > max_len else max_len\n",
    "\n",
    "print(max_len)\n",
    "print(' '.join([bigram for bigram in bigrams_data[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words in Vocabulary: 9315\n"
     ]
    }
   ],
   "source": [
    "total_vocabulary = set(word for text in bigrams_data for word in text)\n",
    "print(f'Total Words in Vocabulary: {len(total_vocabulary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words in Embedding Index: 8116\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    if word in total_vocabulary:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(f'Total Words in Embedding Index: {len(embeddings_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(embeddings_index))])\n",
    "    \n",
    "    # Note from Mike: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # It can't be used in a sklearn Pipeline. \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf  = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(embeddings_index)),\n",
    "                (\"Random Forest\", RandomForestClassifier(n_estimators=100, verbose=True, n_jobs=-1))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(embeddings_index)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr  = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(embeddings_index)),\n",
    "                ('Logistic Regression', LogisticRegression(n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          (\"Support Vector Machine\", svc),\n",
    "          (\"Logistic Regression\", lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "scores = [(name, cross_val_score(model, data, target, cv=5).mean()) for name, model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.8578357355296271),\n",
       " ('Support Vector Machine', 0.770178448867536),\n",
       " ('Logistic Regression', 0.8567833447723633)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, GRU\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing import text\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Tokens: 9315\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(bigrams_data)\n",
    "sequences = tokenizer.texts_to_sequences(bigrams_data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Total Unique Tokens: {len(word_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>244</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>367</td>\n",
       "      <td>744</td>\n",
       "      <td>4629</td>\n",
       "      <td>697</td>\n",
       "      <td>320</td>\n",
       "      <td>1115</td>\n",
       "      <td>905</td>\n",
       "      <td>[job, yes, pay, salary, doesnt, give, meaning,...</td>\n",
       "      <td>[job, that, yes, pay, a, salary, but, doesnt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>775</td>\n",
       "      <td>44</td>\n",
       "      <td>5997</td>\n",
       "      <td>2621</td>\n",
       "      <td>8</td>\n",
       "      <td>746</td>\n",
       "      <td>46</td>\n",
       "      <td>725</td>\n",
       "      <td>[two, buildings, hes, running, around, ohio, t...</td>\n",
       "      <td>[two, buildings, all, the, while, hes, been, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>910</td>\n",
       "      <td>3906</td>\n",
       "      <td>5998</td>\n",
       "      <td>4630</td>\n",
       "      <td>...</td>\n",
       "      <td>604</td>\n",
       "      <td>628</td>\n",
       "      <td>6014</td>\n",
       "      <td>4640</td>\n",
       "      <td>570</td>\n",
       "      <td>362</td>\n",
       "      <td>6015</td>\n",
       "      <td>2626</td>\n",
       "      <td>[rising, tide, hardline, rightwing, nationalis...</td>\n",
       "      <td>[the, rising, tide, of, hardline, rightwing, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1695</td>\n",
       "      <td>1696</td>\n",
       "      <td>93</td>\n",
       "      <td>1184</td>\n",
       "      <td>1879</td>\n",
       "      <td>203</td>\n",
       "      <td>1880</td>\n",
       "      <td>587</td>\n",
       "      <td>[whoa, great, thank_much, backstage, listening...</td>\n",
       "      <td>[whoa, it, is, so, great, to, be, here, thank,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>115</td>\n",
       "      <td>905</td>\n",
       "      <td>148</td>\n",
       "      <td>6034</td>\n",
       "      <td>494</td>\n",
       "      <td>54</td>\n",
       "      <td>42</td>\n",
       "      <td>904</td>\n",
       "      <td>[hello, pittsburgh, whoa, hello, back, thank_t...</td>\n",
       "      <td>[hello, pittsburgh, whoa, hello, back, there, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5    6     7     8     9  \\\n",
       "0  0  0  0  0  0  0    0     0    87   244   \n",
       "1  0  0  0  0  0  0    0     0     0     0   \n",
       "2  0  0  0  0  0  0  910  3906  5998  4630   \n",
       "3  0  0  0  0  0  0    0     0     0     0   \n",
       "4  0  0  0  0  0  0    0     0     0     0   \n",
       "\n",
       "                         ...                           135   136   137   138  \\\n",
       "0                        ...                             2   367   744  4629   \n",
       "1                        ...                           775    44  5997  2621   \n",
       "2                        ...                           604   628  6014  4640   \n",
       "3                        ...                          1695  1696    93  1184   \n",
       "4                        ...                           115   905   148  6034   \n",
       "\n",
       "    139  140   141   142                                            bigrams  \\\n",
       "0   697  320  1115   905  [job, yes, pay, salary, doesnt, give, meaning,...   \n",
       "1     8  746    46   725  [two, buildings, hes, running, around, ohio, t...   \n",
       "2   570  362  6015  2626  [rising, tide, hardline, rightwing, nationalis...   \n",
       "3  1879  203  1880   587  [whoa, great, thank_much, backstage, listening...   \n",
       "4   494   54    42   904  [hello, pittsburgh, whoa, hello, back, thank_t...   \n",
       "\n",
       "                                                text  \n",
       "0  [job, that, yes, pay, a, salary, but, doesnt, ...  \n",
       "1  [two, buildings, all, the, while, hes, been, r...  \n",
       "2  [the, rising, tide, of, hardline, rightwing, n...  \n",
       "3  [whoa, it, is, so, great, to, be, here, thank,...  \n",
       "4  [hello, pittsburgh, whoa, hello, back, there, ...  \n",
       "\n",
       "[5 rows x 145 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t_df = pd.DataFrame(X_t)\n",
    "X_t_df['bigrams'] = bigrams_data\n",
    "X_t_df['text'] = normalized_df.reset_index(drop=True)['text']\n",
    "X_t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_t_df, y, test_size=.5, random_state=0)\n",
    "\n",
    "X_train_text = X_train[['text', 'bigrams']]\n",
    "X_train = X_train[X_train.columns[:-2]]\n",
    "\n",
    "X_test_text = X_test[['text', 'bigrams']]\n",
    "X_test = X_test[X_test.columns[:-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1, \n",
    "                            100, weights=[embedding_matrix], \n",
    "                            input_length = max_len, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(cdim=128, ksize=5, pool1=5, pool2=35, density=128, lr=.001, epochs=100, batch_size=128, validation_split=.3, patience=5, verbose=0):\n",
    "    \n",
    "    # Start Timer\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "    # Display Hyperparameter Settings\n",
    "    print('Convolution Dimensions\\tWindow Size\\tPool 1\\t\\tPool 2\\t\\tDensity')\n",
    "    print(f'{cdim}\\t\\t\\t{ksize}\\t\\t{pool1}\\t\\t{pool2}\\t\\t{density}')\n",
    "    \n",
    "    \n",
    "    # Build Model\n",
    "    input_ = Input(shape=(max_len,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(input_)\n",
    "    \n",
    "    x = Conv1D(cdim, ksize, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(pool1, padding='same')(x)\n",
    "    x = Conv1D(cdim, ksize, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool1, padding='same')(x)\n",
    "    x = Conv1D(cdim, ksize, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool2, padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(density, activation='relu')(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_, outputs=x)\n",
    "    \n",
    "    \n",
    "    # Compile Model\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=Adam(lr=lr), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    # Create Checkpoints & Stopping Parameters\n",
    "    checkpoints_path = f'cnn_best_{selections[0]}_{selections[1]}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(checkpoints_path, \n",
    "                                 monitor='val_loss', \n",
    "                                 verbose=verbose, \n",
    "                                 save_best_only=True, \n",
    "                                 mode='min')\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                   mode='min', \n",
    "                                   patience=patience)\n",
    "    \n",
    "    callbacks = [checkpoint, early_stopping]\n",
    "    \n",
    "    \n",
    "    # Fit Model\n",
    "    model.fit(X_train, y_train, \n",
    "              epochs=epochs, \n",
    "              batch_size=batch_size, \n",
    "              validation_split=validation_split, \n",
    "              callbacks=callbacks, \n",
    "              verbose=verbose)\n",
    "    \n",
    "    # Evaluate Model\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # End Timer\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    # Display Results\n",
    "    print(f'Time to Complete:\\t{end - start}')\n",
    "    print(f'Loss:\\t{results[0]:.2f}\\tAccuracy:\\t{results[1]:.4f}\\n')\n",
    "    \n",
    "    return results, end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 0:00:19.830869\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t3\t\t3\t\t25\t\t64\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "468/468 [==============================] - 0s 203us/step\n",
      "Time to Complete:\t0:00:09.727614\n",
      "Loss:\t0.27\tAccuracy:\t0.8953\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t3\t\t3\t\t25\t\t128\n",
      "468/468 [==============================] - 0s 208us/step\n",
      "Time to Complete:\t0:00:07.613332\n",
      "Loss:\t0.23\tAccuracy:\t0.9167\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t3\t\t3\t\t35\t\t64\n",
      "468/468 [==============================] - 0s 201us/step\n",
      "Time to Complete:\t0:00:07.314602\n",
      "Loss:\t0.21\tAccuracy:\t0.9338\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t3\t\t3\t\t35\t\t128\n",
      "468/468 [==============================] - 0s 220us/step\n",
      "Time to Complete:\t0:00:06.199546\n",
      "Loss:\t0.22\tAccuracy:\t0.9252\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t3\t\t5\t\t25\t\t64\n",
      "468/468 [==============================] - 0s 178us/step\n",
      "Time to Complete:\t0:00:06.211785\n",
      "Loss:\t0.19\tAccuracy:\t0.9444\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t3\t\t5\t\t25\t\t128\n",
      "468/468 [==============================] - 0s 195us/step\n",
      "Time to Complete:\t0:00:06.498077\n",
      "Loss:\t0.19\tAccuracy:\t0.9466\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t3\t\t5\t\t35\t\t64\n",
      "468/468 [==============================] - 0s 174us/step\n",
      "Time to Complete:\t0:00:07.694179\n",
      "Loss:\t0.17\tAccuracy:\t0.9487\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t3\t\t5\t\t35\t\t128\n",
      "468/468 [==============================] - 0s 189us/step\n",
      "Time to Complete:\t0:00:06.034140\n",
      "Loss:\t0.21\tAccuracy:\t0.9402\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t3\t\t7\t\t25\t\t64\n",
      "468/468 [==============================] - 0s 165us/step\n",
      "Time to Complete:\t0:00:06.268431\n",
      "Loss:\t0.17\tAccuracy:\t0.9530\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t3\t\t7\t\t25\t\t128\n",
      "468/468 [==============================] - 0s 171us/step\n",
      "Time to Complete:\t0:00:06.550786\n",
      "Loss:\t0.17\tAccuracy:\t0.9487\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t3\t\t7\t\t35\t\t64\n",
      "468/468 [==============================] - 0s 181us/step\n",
      "Time to Complete:\t0:00:06.641870\n",
      "Loss:\t0.17\tAccuracy:\t0.9487\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t3\t\t7\t\t35\t\t128\n",
      "468/468 [==============================] - 0s 167us/step\n",
      "Time to Complete:\t0:00:06.131455\n",
      "Loss:\t0.17\tAccuracy:\t0.9530\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t5\t\t3\t\t25\t\t64\n",
      "468/468 [==============================] - 0s 299us/step\n",
      "Time to Complete:\t0:00:07.796171\n",
      "Loss:\t0.20\tAccuracy:\t0.9380\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t5\t\t3\t\t25\t\t128\n",
      "468/468 [==============================] - 0s 287us/step\n",
      "Time to Complete:\t0:00:08.070418\n",
      "Loss:\t0.23\tAccuracy:\t0.9615\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t5\t\t3\t\t35\t\t64\n",
      "468/468 [==============================] - 0s 275us/step\n",
      "Time to Complete:\t0:00:09.257338\n",
      "Loss:\t0.17\tAccuracy:\t0.9466\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t5\t\t3\t\t35\t\t128\n",
      "468/468 [==============================] - 0s 300us/step\n",
      "Time to Complete:\t0:00:08.273921\n",
      "Loss:\t0.20\tAccuracy:\t0.9551\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t5\t\t5\t\t25\t\t64\n",
      "468/468 [==============================] - 0s 256us/step\n",
      "Time to Complete:\t0:00:09.132165\n",
      "Loss:\t0.20\tAccuracy:\t0.9509\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t5\t\t5\t\t25\t\t128\n",
      "468/468 [==============================] - 0s 258us/step\n",
      "Time to Complete:\t0:00:07.715644\n",
      "Loss:\t0.24\tAccuracy:\t0.9509\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t5\t\t5\t\t35\t\t64\n",
      "468/468 [==============================] - 0s 254us/step\n",
      "Time to Complete:\t0:00:07.902550\n",
      "Loss:\t0.18\tAccuracy:\t0.9487\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t5\t\t5\t\t35\t\t128\n",
      "468/468 [==============================] - 0s 253us/step\n",
      "Time to Complete:\t0:00:08.678245\n",
      "Loss:\t0.38\tAccuracy:\t0.9231\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t5\t\t7\t\t25\t\t64\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t5\t\t7\t\t25\t\t128\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t5\t\t7\t\t35\t\t64\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t5\t\t7\t\t35\t\t128\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t7\t\t3\t\t25\t\t64\n",
      "468/468 [==============================] - 0s 373us/step\n",
      "Time to Complete:\t0:00:09.709577\n",
      "Loss:\t0.21\tAccuracy:\t0.9551\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t7\t\t3\t\t25\t\t128\n",
      "468/468 [==============================] - 0s 367us/step\n",
      "Time to Complete:\t0:00:09.709982\n",
      "Loss:\t0.33\tAccuracy:\t0.9423\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t7\t\t3\t\t35\t\t64\n",
      "468/468 [==============================] - 0s 382us/step\n",
      "Time to Complete:\t0:00:10.065938\n",
      "Loss:\t0.25\tAccuracy:\t0.9359\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t7\t\t3\t\t35\t\t128\n",
      "468/468 [==============================] - 0s 359us/step\n",
      "Time to Complete:\t0:00:10.231182\n",
      "Loss:\t0.21\tAccuracy:\t0.9637\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t7\t\t5\t\t25\t\t64\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t7\t\t5\t\t25\t\t128\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t7\t\t5\t\t35\t\t64\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t7\t\t5\t\t35\t\t128\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t7\t\t7\t\t25\t\t64\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t7\t\t7\t\t25\t\t128\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t7\t\t7\t\t35\t\t64\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "128\t\t\t7\t\t7\t\t35\t\t128\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t3\t\t3\t\t25\t\t64\n",
      "468/468 [==============================] - 0s 432us/step\n",
      "Time to Complete:\t0:00:13.392946\n",
      "Loss:\t0.12\tAccuracy:\t0.9701\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t3\t\t3\t\t25\t\t128\n",
      "468/468 [==============================] - 0s 407us/step\n",
      "Time to Complete:\t0:00:11.327333\n",
      "Loss:\t0.12\tAccuracy:\t0.9701\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t3\t\t3\t\t35\t\t64\n",
      "468/468 [==============================] - 0s 514us/step\n",
      "Time to Complete:\t0:00:12.827522\n",
      "Loss:\t0.20\tAccuracy:\t0.9274\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t3\t\t3\t\t35\t\t128\n",
      "468/468 [==============================] - 0s 425us/step\n",
      "Time to Complete:\t0:00:11.484006\n",
      "Loss:\t0.17\tAccuracy:\t0.9573\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t3\t\t5\t\t25\t\t64\n",
      "468/468 [==============================] - 0s 339us/step\n",
      "Time to Complete:\t0:00:10.473222\n",
      "Loss:\t0.15\tAccuracy:\t0.9573\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t3\t\t5\t\t25\t\t128\n",
      "468/468 [==============================] - 0s 349us/step\n",
      "Time to Complete:\t0:00:11.480227\n",
      "Loss:\t0.20\tAccuracy:\t0.9359\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t3\t\t5\t\t35\t\t64\n",
      "468/468 [==============================] - 0s 333us/step\n",
      "Time to Complete:\t0:00:11.861960\n",
      "Loss:\t0.11\tAccuracy:\t0.9637\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t3\t\t5\t\t35\t\t128\n",
      "468/468 [==============================] - 0s 331us/step\n",
      "Time to Complete:\t0:00:11.588474\n",
      "Loss:\t0.14\tAccuracy:\t0.9658\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t3\t\t7\t\t25\t\t64\n",
      "468/468 [==============================] - 0s 314us/step\n",
      "Time to Complete:\t0:00:11.294190\n",
      "Loss:\t0.16\tAccuracy:\t0.9615\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t3\t\t7\t\t25\t\t128\n",
      "468/468 [==============================] - 0s 326us/step\n",
      "Time to Complete:\t0:00:11.585298\n",
      "Loss:\t0.17\tAccuracy:\t0.9573\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t3\t\t7\t\t35\t\t64\n",
      "468/468 [==============================] - 0s 316us/step\n",
      "Time to Complete:\t0:00:12.500410\n",
      "Loss:\t0.11\tAccuracy:\t0.9658\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t3\t\t7\t\t35\t\t128\n",
      "468/468 [==============================] - 0s 331us/step\n",
      "Time to Complete:\t0:00:10.745862\n",
      "Loss:\t0.09\tAccuracy:\t0.9765\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t5\t\t3\t\t25\t\t64\n",
      "468/468 [==============================] - 0s 626us/step\n",
      "Time to Complete:\t0:00:17.225784\n",
      "Loss:\t0.14\tAccuracy:\t0.9679\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t5\t\t3\t\t25\t\t128\n",
      "468/468 [==============================] - 0s 590us/step\n",
      "Time to Complete:\t0:00:13.700606\n",
      "Loss:\t0.17\tAccuracy:\t0.9573\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t5\t\t3\t\t35\t\t64\n",
      "468/468 [==============================] - 0s 601us/step\n",
      "Time to Complete:\t0:00:14.676580\n",
      "Loss:\t0.17\tAccuracy:\t0.9594\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t5\t\t3\t\t35\t\t128\n",
      "468/468 [==============================] - 0s 604us/step\n",
      "Time to Complete:\t0:00:14.418854\n",
      "Loss:\t0.36\tAccuracy:\t0.9338\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t5\t\t5\t\t25\t\t64\n",
      "468/468 [==============================] - 0s 493us/step\n",
      "Time to Complete:\t0:00:14.040669\n",
      "Loss:\t0.21\tAccuracy:\t0.9615\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t5\t\t5\t\t25\t\t128\n",
      "468/468 [==============================] - 0s 483us/step\n",
      "Time to Complete:\t0:00:14.169858\n",
      "Loss:\t0.23\tAccuracy:\t0.9637\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t5\t\t5\t\t35\t\t64\n",
      "468/468 [==============================] - 0s 505us/step\n",
      "Time to Complete:\t0:00:13.899123\n",
      "Loss:\t0.17\tAccuracy:\t0.9551\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t5\t\t5\t\t35\t\t128\n",
      "468/468 [==============================] - 0s 500us/step\n",
      "Time to Complete:\t0:00:12.781175\n",
      "Loss:\t0.17\tAccuracy:\t0.9637\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t5\t\t7\t\t25\t\t64\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t5\t\t7\t\t25\t\t128\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t5\t\t7\t\t35\t\t64\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t5\t\t7\t\t35\t\t128\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t7\t\t3\t\t25\t\t64\n",
      "468/468 [==============================] - 0s 735us/step\n",
      "Time to Complete:\t0:00:17.456385\n",
      "Loss:\t0.66\tAccuracy:\t0.8718\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t7\t\t3\t\t25\t\t128\n",
      "468/468 [==============================] - 0s 746us/step\n",
      "Time to Complete:\t0:00:16.649321\n",
      "Loss:\t0.29\tAccuracy:\t0.9359\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t7\t\t3\t\t35\t\t64\n",
      "468/468 [==============================] - 0s 779us/step\n",
      "Time to Complete:\t0:00:19.304479\n",
      "Loss:\t0.21\tAccuracy:\t0.9701\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t7\t\t3\t\t35\t\t128\n",
      "468/468 [==============================] - 0s 746us/step\n",
      "Time to Complete:\t0:00:17.863249\n",
      "Loss:\t0.35\tAccuracy:\t0.9380\n",
      "\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t7\t\t5\t\t25\t\t64\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t7\t\t5\t\t25\t\t128\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t7\t\t5\t\t35\t\t64\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t7\t\t5\t\t35\t\t128\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t7\t\t7\t\t25\t\t64\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t7\t\t7\t\t25\t\t128\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t7\t\t7\t\t35\t\t64\n",
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t7\t\t7\t\t35\t\t128\n",
      "Time Elapsed: 0:08:57.467112\n",
      "\n",
      "Time to Find Best Model: 0:08:37.636243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = datetime.datetime.now()\n",
    "print(f'Time Elapsed: {t1 - t0}\\n')\n",
    "\n",
    "cdims = [128, 256]\n",
    "ksizes = [3, 5, 7]\n",
    "pools1 = [3, 5, 7]\n",
    "pools2 = [25, 35]\n",
    "densities = [64, 128]\n",
    "\n",
    "cnn_results = {'Convolution Dimensions': [], \n",
    "               'Window Size': [], \n",
    "               'Pool 1': [], \n",
    "               'Pool 2': [], \n",
    "               'Density': [], \n",
    "               'Time': [], \n",
    "               'Loss': [], \n",
    "               'Accuracy': []}\n",
    "\n",
    "for cdim in cdims:\n",
    "    for ksize in ksizes:\n",
    "        for pool1 in pools1:\n",
    "            for pool2 in pools2:\n",
    "                for density in densities:\n",
    "                    try:\n",
    "                        cnn_eval, cnn_time = create_cnn_model(cdim=cdim, ksize=ksize, pool1=pool1, pool2=pool2, density=density)\n",
    "                        cnn_results['Convolution Dimensions'].append(cdim)\n",
    "                        cnn_results['Window Size'].append(ksize)\n",
    "                        cnn_results['Pool 1'].append(pool1)\n",
    "                        cnn_results['Pool 2'].append(pool2)\n",
    "                        cnn_results['Density'].append(density)\n",
    "                        cnn_results['Loss'].append(cnn_eval[0])\n",
    "                        cnn_results['Accuracy'].append(cnn_eval[1])\n",
    "                        cnn_results['Time'].append(cnn_time)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "t2 = datetime.datetime.now()\n",
    "print(f'Time Elapsed: {t2 - t0}\\n')\n",
    "print(f'Time to Find Best Model: {t2 - t1}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Dimensions\tWindow Size\tPool 1\t\tPool 2\t\tDensity\n",
      "256\t\t\t3\t\t7\t\t35\t\t128\n",
      "Train on 327 samples, validate on 141 samples\n",
      "Epoch 1/100\n",
      "327/327 [==============================] - 4s 13ms/step - loss: 0.6850 - acc: 0.5780 - val_loss: 0.6485 - val_acc: 0.7021\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64847, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 2/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.5642 - acc: 0.9327 - val_loss: 0.5830 - val_acc: 0.8723\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64847 to 0.58301, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 3/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.4631 - acc: 0.9908 - val_loss: 0.5236 - val_acc: 0.9220\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.58301 to 0.52360, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 4/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.3736 - acc: 1.0000 - val_loss: 0.4677 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52360 to 0.46767, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 5/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.2906 - acc: 1.0000 - val_loss: 0.4066 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.46767 to 0.40658, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 6/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.2170 - acc: 1.0000 - val_loss: 0.3495 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.40658 to 0.34950, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 7/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.1546 - acc: 1.0000 - val_loss: 0.3027 - val_acc: 0.9291\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.34950 to 0.30268, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 8/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.1034 - acc: 1.0000 - val_loss: 0.2546 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.30268 to 0.25459, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 9/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0682 - acc: 1.0000 - val_loss: 0.2195 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.25459 to 0.21951, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 10/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0439 - acc: 1.0000 - val_loss: 0.1976 - val_acc: 0.9433\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.21951 to 0.19755, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 11/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0289 - acc: 1.0000 - val_loss: 0.1762 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.19755 to 0.17617, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 12/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.1577 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.17617 to 0.15774, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 13/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.1462 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.15774 to 0.14616, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 14/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.1388 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.14616 to 0.13882, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 15/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0065 - acc: 1.0000 - val_loss: 0.1341 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.13882 to 0.13415, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 16/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.1297 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.13415 to 0.12972, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 17/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.1262 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.12972 to 0.12624, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 18/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.1228 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.12624 to 0.12284, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 19/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.1200 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.12284 to 0.12004, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 20/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.1185 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.12004 to 0.11849, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 21/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.1177 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.11849 to 0.11768, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 22/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.1172 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.11768 to 0.11723, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 23/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.1171 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.11723 to 0.11711, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 24/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.1169 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.11711 to 0.11687, saving model to cnn_best_trump_hclinton.hdf5\n",
      "Epoch 25/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.1170 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.11687\n",
      "Epoch 26/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.1174 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.11687\n",
      "Epoch 27/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 9.5897e-04 - acc: 1.0000 - val_loss: 0.1175 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.11687\n",
      "Epoch 28/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 8.8739e-04 - acc: 1.0000 - val_loss: 0.1175 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.11687\n",
      "Epoch 29/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 8.2369e-04 - acc: 1.0000 - val_loss: 0.1176 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.11687\n",
      "468/468 [==============================] - 0s 349us/step\n",
      "Time to Complete:\t0:00:21.409846\n",
      "Loss:\t0.09\tAccuracy:\t0.9679\n",
      "\n",
      "Time Elapsed: 0:09:18.901003\n",
      "\n",
      "Time to Train Best Model: 0:00:21.433891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_results_df = pd.DataFrame.from_dict(cnn_results)\n",
    "\n",
    "best_cnn_model = cnn_results_df[cnn_results_df['Accuracy'] == cnn_results_df['Accuracy'].max()]\n",
    "\n",
    "cdim = best_cnn_model['Convolution Dimensions'].values[0]\n",
    "ksize = int(best_cnn_model['Window Size'].values[0])\n",
    "pool1 = int(best_cnn_model['Pool 1'].values[0])\n",
    "pool2 = int(best_cnn_model['Pool 2'].values[0])\n",
    "density = int(best_cnn_model['Density'].values[0])\n",
    "\n",
    "cnn_eval, cnn_time = create_cnn_model(cdim=cdim, \n",
    "                                      ksize=ksize, \n",
    "                                      pool1=pool1, \n",
    "                                      pool2=pool2, \n",
    "                                      density=density,\n",
    "                                      lr=.0001, \n",
    "                                      epochs=100, \n",
    "                                      patience=5, \n",
    "                                      verbose=1)\n",
    "\n",
    "t3 = datetime.datetime.now()\n",
    "print(f'Time Elapsed: {t3 - t0}\\n')\n",
    "print(f'Time to Train Best Model: {t3 - t2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(rnn_type='gru', units=50, drop=.5, density=50, lr=.001, epochs=100, batch_size=128, validation_split=.3, patience=5, verbose=0):\n",
    "    \n",
    "    # Start Timer\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "    # Display Hyperparameter Settings\n",
    "    model_type = 'GRU' if rnn_type == 'gru' else 'LSTM'\n",
    "    print(f'Model Type:\\t{model_type}')\n",
    "    print(f'Batch Size:\\t{batch_size}\\tUnits:\\t{units}\\tDropout Rate:\\t{drop}\\t\\tDensity: {density}')\n",
    "    \n",
    "    \n",
    "    # Build Model\n",
    "    input_ = Input(shape=(max_len,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(input_)\n",
    "    \n",
    "    if rnn_type == 'gru':\n",
    "        x = Bidirectional(GRU(units, return_sequences=True))(embedded_sequences)\n",
    "    else:\n",
    "        x = Bidirectional(LSTM(units, return_sequences=True))(embedded_sequences)\n",
    "        \n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    x = Dense(density, activation='relu')(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_, outputs=x)\n",
    "    \n",
    "    \n",
    "    # Compile Model\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=Adam(lr=lr), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    # Create Checkpoints & Stopping Parameters\n",
    "    checkpoints_path = f'rnn_best_{selections[0]}_{selections[1]}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(checkpoints_path, \n",
    "                                 monitor='val_loss', \n",
    "                                 verbose=verbose, \n",
    "                                 save_best_only=True, \n",
    "                                 mode='min')\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                   mode='min', \n",
    "                                   patience=patience)\n",
    "    \n",
    "    callbacks = [checkpoint, early_stopping]\n",
    "    \n",
    "    \n",
    "    # Fit Model\n",
    "    model.fit(X_train, y_train, \n",
    "              epochs=epochs, \n",
    "              batch_size=batch_size, \n",
    "              validation_split=validation_split, \n",
    "              callbacks=callbacks, \n",
    "              verbose=verbose)\n",
    "    \n",
    "    # Evaluate Model\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # End Timer\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    # Display Results\n",
    "    print(f'Time to Complete:\\t{end - start}')\n",
    "    print(f'Loss:\\t{results[0]:.2f}\\tAccuracy:\\t{results[1]:.4f}\\n')\n",
    "    \n",
    "    return results, end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Type:\tLSTM\n",
      "Batch Size:\t128\tUnits:\t25\tDropout Rate:\t0.25\t\tDensity: 100\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "468/468 [==============================] - 0s 644us/step\n",
      "Time to Complete:\t0:00:24.442185\n",
      "Loss:\t0.07\tAccuracy:\t0.9786\n",
      "\n",
      "Model Type:\tLSTM\n",
      "Batch Size:\t128\tUnits:\t25\tDropout Rate:\t0.25\t\tDensity: 150\n",
      "468/468 [==============================] - 0s 590us/step\n",
      "Time to Complete:\t0:00:21.845292\n",
      "Loss:\t0.07\tAccuracy:\t0.9850\n",
      "\n",
      "Model Type:\tLSTM\n",
      "Batch Size:\t128\tUnits:\t25\tDropout Rate:\t0.5\t\tDensity: 100\n",
      "468/468 [==============================] - 0s 596us/step\n",
      "Time to Complete:\t0:00:24.294935\n",
      "Loss:\t0.08\tAccuracy:\t0.9722\n",
      "\n",
      "Model Type:\tLSTM\n",
      "Batch Size:\t128\tUnits:\t25\tDropout Rate:\t0.5\t\tDensity: 150\n",
      "468/468 [==============================] - 0s 599us/step\n",
      "Time to Complete:\t0:00:26.039041\n",
      "Loss:\t0.07\tAccuracy:\t0.9808\n",
      "\n",
      "Model Type:\tLSTM\n",
      "Batch Size:\t128\tUnits:\t50\tDropout Rate:\t0.25\t\tDensity: 100\n",
      "468/468 [==============================] - 0s 657us/step\n",
      "Time to Complete:\t0:00:24.635805\n",
      "Loss:\t0.09\tAccuracy:\t0.9722\n",
      "\n",
      "Model Type:\tLSTM\n",
      "Batch Size:\t128\tUnits:\t50\tDropout Rate:\t0.25\t\tDensity: 150\n",
      "468/468 [==============================] - 0s 666us/step\n",
      "Time to Complete:\t0:00:22.858192\n",
      "Loss:\t0.07\tAccuracy:\t0.9829\n",
      "\n",
      "Model Type:\tLSTM\n",
      "Batch Size:\t128\tUnits:\t50\tDropout Rate:\t0.5\t\tDensity: 100\n",
      "468/468 [==============================] - 0s 669us/step\n",
      "Time to Complete:\t0:00:26.397554\n",
      "Loss:\t0.11\tAccuracy:\t0.9786\n",
      "\n",
      "Model Type:\tLSTM\n",
      "Batch Size:\t128\tUnits:\t50\tDropout Rate:\t0.5\t\tDensity: 150\n",
      "468/468 [==============================] - 0s 668us/step\n",
      "Time to Complete:\t0:00:25.398088\n",
      "Loss:\t0.10\tAccuracy:\t0.9701\n",
      "\n",
      "Model Type:\tGRU\n",
      "Batch Size:\t128\tUnits:\t25\tDropout Rate:\t0.25\t\tDensity: 100\n",
      "468/468 [==============================] - 0s 568us/step\n",
      "Time to Complete:\t0:00:32.659584\n",
      "Loss:\t0.08\tAccuracy:\t0.9744\n",
      "\n",
      "Model Type:\tGRU\n",
      "Batch Size:\t128\tUnits:\t25\tDropout Rate:\t0.25\t\tDensity: 150\n",
      "468/468 [==============================] - 0s 575us/step\n",
      "Time to Complete:\t0:00:43.219741\n",
      "Loss:\t0.07\tAccuracy:\t0.9765\n",
      "\n",
      "Model Type:\tGRU\n",
      "Batch Size:\t128\tUnits:\t25\tDropout Rate:\t0.5\t\tDensity: 100\n",
      "468/468 [==============================] - 0s 608us/step\n",
      "Time to Complete:\t0:00:38.012112\n",
      "Loss:\t0.07\tAccuracy:\t0.9722\n",
      "\n",
      "Model Type:\tGRU\n",
      "Batch Size:\t128\tUnits:\t25\tDropout Rate:\t0.5\t\tDensity: 150\n",
      "468/468 [==============================] - 0s 564us/step\n",
      "Time to Complete:\t0:00:31.267714\n",
      "Loss:\t0.09\tAccuracy:\t0.9722\n",
      "\n",
      "Model Type:\tGRU\n",
      "Batch Size:\t128\tUnits:\t50\tDropout Rate:\t0.25\t\tDensity: 100\n",
      "468/468 [==============================] - 0s 596us/step\n",
      "Time to Complete:\t0:00:30.413855\n",
      "Loss:\t0.09\tAccuracy:\t0.9722\n",
      "\n",
      "Model Type:\tGRU\n",
      "Batch Size:\t128\tUnits:\t50\tDropout Rate:\t0.25\t\tDensity: 150\n",
      "468/468 [==============================] - 0s 606us/step\n",
      "Time to Complete:\t0:00:40.548500\n",
      "Loss:\t0.06\tAccuracy:\t0.9808\n",
      "\n",
      "Model Type:\tGRU\n",
      "Batch Size:\t128\tUnits:\t50\tDropout Rate:\t0.5\t\tDensity: 100\n",
      "468/468 [==============================] - 0s 596us/step\n",
      "Time to Complete:\t0:00:39.086825\n",
      "Loss:\t0.07\tAccuracy:\t0.9808\n",
      "\n",
      "Model Type:\tGRU\n",
      "Batch Size:\t128\tUnits:\t50\tDropout Rate:\t0.5\t\tDensity: 150\n",
      "468/468 [==============================] - 0s 571us/step\n",
      "Time to Complete:\t0:00:31.711694\n",
      "Loss:\t0.09\tAccuracy:\t0.9765\n",
      "\n",
      "Time Elapsed: 0:17:21.755267\n",
      "\n",
      "Time to Find Best Model: 0:08:02.854264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_types = ['lstm', 'gru']\n",
    "units = [25, 50]\n",
    "drops = [.25, .5]\n",
    "densities = [100, 150]\n",
    "\n",
    "rnn_results = {'Model Type': [], 'Units': [], 'Dropout Rate': [], 'Density': [], 'Time': [], 'Loss': [], 'Accuracy': []}\n",
    "\n",
    "for rnn_type in rnn_types:\n",
    "    for unit in units:\n",
    "        for drop in drops:\n",
    "            for density in densities:\n",
    "                rnn_eval, rnn_time = create_rnn_model(rnn_type=rnn_type, units=unit, drop=drop, density=density)\n",
    "                rnn_results['Model Type'].append(rnn_type)\n",
    "                rnn_results['Units'].append(unit)\n",
    "                rnn_results['Dropout Rate'].append(drop)\n",
    "                rnn_results['Density'].append(density)\n",
    "                rnn_results['Loss'].append(rnn_eval[0])\n",
    "                rnn_results['Accuracy'].append(rnn_eval[1])\n",
    "                rnn_results['Time'].append(rnn_time)\n",
    "                \n",
    "t4 = datetime.datetime.now()\n",
    "print(f'Time Elapsed: {t4 - t0}\\n')\n",
    "print(f'Time to Find Best Model: {t4 - t3}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Type:\tGRU\n",
      "Batch Size:\t128\tUnits:\t50\tDropout Rate:\t0.25\t\tDensity: 150\n",
      "Train on 327 samples, validate on 141 samples\n",
      "Epoch 1/100\n",
      "327/327 [==============================] - 9s 28ms/step - loss: 0.7538 - acc: 0.4648 - val_loss: 0.6150 - val_acc: 0.8723\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61502, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 2/100\n",
      "327/327 [==============================] - 1s 2ms/step - loss: 0.5659 - acc: 0.7187 - val_loss: 0.5317 - val_acc: 0.9433\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61502 to 0.53172, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 3/100\n",
      "327/327 [==============================] - 1s 2ms/step - loss: 0.4342 - acc: 0.9083 - val_loss: 0.4529 - val_acc: 0.9007\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53172 to 0.45289, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 4/100\n",
      "327/327 [==============================] - 0s 2ms/step - loss: 0.3179 - acc: 0.9572 - val_loss: 0.3531 - val_acc: 0.9716\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.45289 to 0.35313, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 5/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.2128 - acc: 0.9847 - val_loss: 0.2680 - val_acc: 0.9716\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35313 to 0.26797, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 6/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.1229 - acc: 1.0000 - val_loss: 0.2005 - val_acc: 0.9716\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26797 to 0.20051, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 7/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0650 - acc: 1.0000 - val_loss: 0.1549 - val_acc: 0.9716\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.20051 to 0.15487, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 8/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0354 - acc: 1.0000 - val_loss: 0.1260 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.15487 to 0.12596, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 9/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.1068 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12596 to 0.10680, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 10/100\n",
      "327/327 [==============================] - 0s 2ms/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.0932 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.10680 to 0.09322, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 11/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0855 - val_acc: 0.9716\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09322 to 0.08554, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 12/100\n",
      "327/327 [==============================] - 1s 2ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.08554 to 0.08189, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 13/100\n",
      "327/327 [==============================] - 1s 2ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0802 - val_acc: 0.9716\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08189 to 0.08024, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 14/100\n",
      "327/327 [==============================] - 0s 2ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0789 - val_acc: 0.9716\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.08024 to 0.07895, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 15/100\n",
      "327/327 [==============================] - 1s 2ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0774 - val_acc: 0.9716\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.07895 to 0.07737, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 16/100\n",
      "327/327 [==============================] - 0s 2ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0759 - val_acc: 0.9716\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.07737 to 0.07594, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 17/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 8.2410e-04 - acc: 1.0000 - val_loss: 0.0748 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.07594 to 0.07483, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 18/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 8.7501e-04 - acc: 1.0000 - val_loss: 0.0739 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.07483 to 0.07388, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 19/100\n",
      "327/327 [==============================] - 0s 2ms/step - loss: 6.9087e-04 - acc: 1.0000 - val_loss: 0.0730 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.07388 to 0.07302, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 20/100\n",
      "327/327 [==============================] - 0s 2ms/step - loss: 5.4787e-04 - acc: 1.0000 - val_loss: 0.0724 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.07302 to 0.07242, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 21/100\n",
      "327/327 [==============================] - 0s 2ms/step - loss: 5.0839e-04 - acc: 1.0000 - val_loss: 0.0720 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.07242 to 0.07196, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 22/100\n",
      "327/327 [==============================] - 1s 2ms/step - loss: 4.6345e-04 - acc: 1.0000 - val_loss: 0.0717 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.07196 to 0.07166, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 23/100\n",
      "327/327 [==============================] - 0s 2ms/step - loss: 4.4397e-04 - acc: 1.0000 - val_loss: 0.0714 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.07166 to 0.07144, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 24/100\n",
      "327/327 [==============================] - 0s 2ms/step - loss: 3.7007e-04 - acc: 1.0000 - val_loss: 0.0712 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.07144 to 0.07121, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 25/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 4.0749e-04 - acc: 1.0000 - val_loss: 0.0710 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.07121 to 0.07103, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 26/100\n",
      "327/327 [==============================] - 1s 2ms/step - loss: 3.8260e-04 - acc: 1.0000 - val_loss: 0.0709 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.07103 to 0.07089, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 27/100\n",
      "327/327 [==============================] - 0s 2ms/step - loss: 3.0681e-04 - acc: 1.0000 - val_loss: 0.0708 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.07089 to 0.07076, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 28/100\n",
      "327/327 [==============================] - 1s 2ms/step - loss: 3.8526e-04 - acc: 1.0000 - val_loss: 0.0707 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.07076 to 0.07067, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 29/100\n",
      "327/327 [==============================] - 1s 2ms/step - loss: 3.9184e-04 - acc: 1.0000 - val_loss: 0.0706 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.07067 to 0.07058, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 30/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 3.1998e-04 - acc: 1.0000 - val_loss: 0.0706 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.07058 to 0.07056, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 31/100\n",
      "327/327 [==============================] - 0s 2ms/step - loss: 4.6714e-04 - acc: 1.0000 - val_loss: 0.0705 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.07056 to 0.07049, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 32/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 2.7972e-04 - acc: 1.0000 - val_loss: 0.0703 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.07049 to 0.07031, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 33/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 3.8930e-04 - acc: 1.0000 - val_loss: 0.0702 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.07031 to 0.07024, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 34/100\n",
      "327/327 [==============================] - 0s 2ms/step - loss: 2.0898e-04 - acc: 1.0000 - val_loss: 0.0702 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.07024 to 0.07021, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 35/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 2.8359e-04 - acc: 1.0000 - val_loss: 0.0701 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.07021 to 0.07014, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 36/100\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 2.3175e-04 - acc: 1.0000 - val_loss: 0.0701 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.07014 to 0.07012, saving model to rnn_best_trump_hclinton.hdf5\n",
      "Epoch 37/100\n",
      "327/327 [==============================] - 0s 2ms/step - loss: 3.0791e-04 - acc: 1.0000 - val_loss: 0.0702 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.07012\n",
      "Epoch 38/100\n",
      "327/327 [==============================] - 1s 2ms/step - loss: 2.1373e-04 - acc: 1.0000 - val_loss: 0.0703 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.07012\n",
      "Epoch 39/100\n",
      "327/327 [==============================] - 0s 2ms/step - loss: 2.9238e-04 - acc: 1.0000 - val_loss: 0.0705 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.07012\n",
      "Epoch 40/100\n",
      "327/327 [==============================] - 1s 2ms/step - loss: 3.3899e-04 - acc: 1.0000 - val_loss: 0.0708 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.07012\n",
      "Epoch 41/100\n",
      "327/327 [==============================] - 1s 2ms/step - loss: 3.9345e-04 - acc: 1.0000 - val_loss: 0.0713 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.07012\n",
      "468/468 [==============================] - 0s 607us/step\n",
      "Time to Complete:\t0:00:44.176433\n",
      "Loss:\t0.07\tAccuracy:\t0.9786\n",
      "\n",
      "Time Elapsed: 0:18:05.956414\n",
      "\n",
      "Time to Train Best Model: 0:00:44.201147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_results_df = pd.DataFrame.from_dict(rnn_results)\n",
    "\n",
    "best_rnn_model = rnn_results_df[rnn_results_df['Loss'] == rnn_results_df['Loss'].min()]\n",
    "\n",
    "rnn_type = best_rnn_model['Model Type'].values[0]\n",
    "units = int(best_rnn_model['Units'].values[0])\n",
    "drop = float(best_rnn_model['Dropout Rate'].values[0])\n",
    "density = int(best_rnn_model['Density'].values[0])\n",
    "\n",
    "rnn_eval, rnn_time = create_rnn_model(rnn_type=rnn_type, \n",
    "                                      units=units, \n",
    "                                      drop=drop, \n",
    "                                      density=density, \n",
    "                                      lr=.001, \n",
    "                                      epochs=100, \n",
    "                                      patience=5, \n",
    "                                      verbose=1)\n",
    "\n",
    "t5 = datetime.datetime.now()\n",
    "print(f'Time Elapsed: {t5 - t0}\\n')\n",
    "print(f'Time to Train Best Model: {t5 - t4}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x1a93447d68>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = 'c' if cnn_eval[1] > rnn_eval[1] else 'r'\n",
    "best_model = load_model(f'{best}nn_best_{selections[0]}_{selections[1]}.hdf5')\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(best_model.predict(X_test), axis=1)\n",
    "actual = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "F1 Score: 97.78%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(actual, preds)\n",
    "cm_plot_labels = [selections[1].title(), selections[0].title()]\n",
    "\n",
    "plot_confusion_matrix(cm, cm_plot_labels, normalize=False, title=f'{best.title()}NN Confusion Matrix')\n",
    "\n",
    "print(f'F1 Score: {f1_score(actual, preds)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Actual</th>\n",
       "      <th>text</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[a, wall, and, mexico, will, be, paying, for, ...</td>\n",
       "      <td>[wall, mexico, paying, wall, great, crowd, ill...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[bob, gates, who, served, eight, presidents, o...</td>\n",
       "      <td>[bob, gates, served, eight, presidents, 50_yea...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[theres, never, been, the, crowds, theres, nev...</td>\n",
       "      <td>[theres_never, crowds, theres_never, enthusias...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[law, and, order, without, it, we, have, nothi...</td>\n",
       "      <td>[law, order, without, nothing, let, take, oppo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, know, what, we, can, do, together, but, ho...</td>\n",
       "      <td>[know, together, going, pay, well, ill_tell, g...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predictions  Actual                                               text  \\\n",
       "0            1       1  [a, wall, and, mexico, will, be, paying, for, ...   \n",
       "1            0       0  [bob, gates, who, served, eight, presidents, o...   \n",
       "2            1       1  [theres, never, been, the, crowds, theres, nev...   \n",
       "3            1       1  [law, and, order, without, it, we, have, nothi...   \n",
       "4            0       0  [i, know, what, we, can, do, together, but, ho...   \n",
       "\n",
       "                                             bigrams  Accuracy  \n",
       "0  [wall, mexico, paying, wall, great, crowd, ill...      True  \n",
       "1  [bob, gates, served, eight, presidents, 50_yea...      True  \n",
       "2  [theres_never, crowds, theres_never, enthusias...      True  \n",
       "3  [law, order, without, nothing, let, take, oppo...      True  \n",
       "4  [know, together, going, pay, well, ill_tell, g...      True  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df = pd.DataFrame.from_dict({'Predictions': preds, 'Actual': actual})\n",
    "preds_df = preds_df.join(X_test_text.reset_index(drop=True))\n",
    "preds_df['Accuracy'] = preds_df['Predictions'] == preds_df['Actual']\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump:\n",
      "thank you very much it is a great honor and i am very very grateful being here today in front of so many distinguished people ive met more generals than ive ever seen in my life i like them and they like me a lost endorsements i can tell you my special gratitude to general ashenhurst and general hargett and to each and everyone of you thank you very much thank you guys the national guard of the united states has defended this nation at war kept our citizens safe at home and rushed into danger wherever it has threatened our people our debt to you is eternal yesterday was the 15th anniversary of the 911 attacks the national guard was sent to assist in the aid and recovery and these are serious serious bad days for our country the recovery efforts at ground zero in new york city and i was there and i know what you went through since 911 there have been 780000 members of the national guard deployed overseas including those with multiple deployments right now 10000 members of the national guard are in iraq afghanistan and many other countries all across the world and doing an unbelievable job members of the national guard come from all walks of life working every kind of job imaginable and juggle a thousand different challenges as they raise their kids serve their states and answer the call of duty for their country i am given the honor to serve \n",
      "\n",
      "Trump:\n",
      "what he was able to do can you imagine that this man singlehandedly he was having problems with churches and there was a church in houston that was giving him a hard time maybe for a very good reason and he put in an amendment that basically stopped our great pastors and ministers and others from talking under the penalty of losing their taxexempt status so we were looking down onto the sidewalk and there were people walking on the sidewalk and i said so folks what youre telling me is those people walking way way down there on the sidewalk have really more power than you do because theyre allowed to express their feelings and thoughts openly and without penalty and one of the pastors who i knew very well and these are powerful people these are strong people with magnificent voices and just and magnificent hearts much more importantly they looked at me and they said thats actually right they have more power than we do were not allowed to express and thats when i said we have to start thinking about this and i thought about it and then we had a large group of pastors again i said i have thought about it if i become president we are going to knock out the johnson amendment we are going to do that and its not going to be hard its not going to be hard when you when you think from the standpoint of political you \n",
      "\n",
      "Trump:\n",
      "the nation and to imagine what we can accomplish if we work together trust each other and put the needs of our citizens first for a change we must break our ties with the failed and bitter politics and policies of the past and pursue a future where every american is honored and i mean really honored and respected the average americans in washington dc that looks down on everyday hardworking people and thats whats happening too often those who have power have disdain for the views beliefs and attitudes of those who dont have any political power those people must put themselves in the shoes of the radio factory worker the family worried about security or the mom struggling to afford child care child care is such a big problem were going to solve that problem that means we need working mothers to be fairly compensated for their work and have access to affordable quality child care for their kids we want higher pay better wages and a growing economy for everyone these solutions must update laws passed more than a half a century ago when most women were still not in the labor force or some of them werent even close today nearly two in three mothers with young children have jobs for many families in our country child care is now the single largest expense who would think that even more so than housing while critical meaningful policy work has been done in this area my opponent \n",
      "\n",
      "Trump:\n",
      "navy is the smallest its been since world war i think of that smallest since world war i and rebuilding it will require a national effort thats why i will ask my secretary of the navy to review facilities like the philadelphia navy yard i know it well with a long history of service to our navy proximity to vibrant private industry and room for expansion to enlist them in the rebuilding of what will be a great fleet again to recruit master craftsman this buildup requires and we will establish centers of excellence at locations like the philadelphia navy yard vocational training is a great thing we dont do it anymore we dont do it anymore and i must tell you so often in going to college right down the road and going to college we have people that were brilliant and we had other people that werent as brilliant in that way but were brilliant incredible when it came to fixing a motor when it came to fixing something that i had no idea what was happening and other great geniuses had no idea what was happening and they could take it apart blindfolded and we dont do that anymore vocational training for our country is so so so important and just in closing and they loved it they loved it they didnt want to be doing what i was doing they loved it they were so good at it and they loved it so vocational training were \n",
      "\n",
      "Trump:\n",
      "we want to achieve a stable peaceful world with less conflict and more common ground i am proposing a new foreign policy focused on advancing americas core national interests so important promoting regional stability and producing and easing the tensions within our very troubled world this will require rethinking the failed policies of the past we can make new friends rebuild old alliances and bring new allies into the fold and we can do that im proud to have the support of warfighting generals active duty military and top experts who know both how to win and how to avoid endless wars that were caught up in like the one we have right now that just never ever ends our longest war just yesterday 88 top generals and admirals endorsed my campaign and these people are fantastic in a trump administration our actions in the middle east will be tempered by realism the current strategy of toppling regimes with no plan for what to do in the day after only produces power vacuums that are filled simply by terrorists gradual reform not sudden and radical change should be our guiding objective in that region we should work with any country that shares our goal of destroying isis and defeat radical islamic terrorism and were going to form new friendships and partnerships based on this mission and this mission alone we now have an administration and a former secretary of state who refuse to say radical islamic terrorism and unless youre \n",
      "\n",
      "Trump:\n",
      "all over the place and it wasnt even and it was like this it was a big level so when they put the water on on one side they had like a foot and a half of water and the other one they had like an inch and they didnt know what they were doing and i said to ed koch and it went to the newspapers i said you know because they didnt want to let me do it they didnt want to be stood up or they didnt want to be shown up and they went i went to the newspapers the editorial board who actually even though im much more successful now they dont treat me nearly as well im trying to figure this out but they actually did treat me very well and they said let trump do it and there was a lot of pressure put on and they let me do it and i did it in four months they had spent i think probably 20 21 million sometimes i hear 18 i hear 17 honestly they had no idea what they spent whatever it was they spent a fortune and i took it over and the first thing i did is i had to get an engineer so i fired the guy from miami because he obviously didnt know what he was doing and he recommended actually he said why dont you try somebody mr trump from montreal i said thats good and \n",
      "\n",
      "Trump:\n",
      "our nation their unbreakable faith and spirit overcame some of the most difficult periods in our history leading us all to a better future its very true amazing this was such an amazing experience this is the power of faith its the power to heal its the power to unite its the power to make all of us live better lives all of us our nation today is divided nobody likes to say it but were living in a very very divided nation it will be our faith in god in his teachings in each other that will lead us back to unity each of us here today has a role to play in bringing our country together united in common purpose and in common values so lets talk today about some of the things and these are great things that we can do together to create the american future for everybody not just a certain group of people but for everybody the first thing we have to do is give our churches their voice back its been taken away the johnson amendment has blocked our pastors and ministers and others from speaking their minds from their own pulpits if they want to talk about christianity if they want to preach if they want to talk about politics theyre unable to do so if they want to do it they take a tremendous risk that they lose their taxexempt status all religious leaders should be able to freely express their \n",
      "\n",
      "Hclinton:\n",
      "you tim and i were in pittsburgh last month with a big crowd and we couldnt get everybody in like today so tim went out to say hello to people and he saw a woman holding a baby probably about three years old and he stopped to talk to her and she said i came here because i wanted to tell you and to tell hillary clinton what happened to me she said that i had my baby it was a hard labor and delivery and i needed some time off and i called by boss and said the doctors want me to take some time off and so i wont be in for i think she said two weeks and they said ok youre fired that is legal in america still and what that does is to say that wait a minute do we really value children do we really value families and weve got to have a system that is fair to employers and fair to employees because you got to be able to balance work and family in todays america and then finally i think it is way past time to guarantee equal pay for womens work and you know i gotta say this always gets a huge round of applause as you can tell but this is not just a womans issue its a family issue if you have a wife a mother a sister a daughter whos in the work force you want her to \n",
      "\n",
      "Trump:\n",
      "thank you thank you thank you very much thank you and its great to be in charlotte i just met with our many amazing employees right up the road at my property i will tell you they like me very much i guess i pay them a little too much id like to take a moment to talk about the heartbreak and devastation in louisiana a state that is very very special to me we are one nation when one state hurts we all hurt and we must all work together to lift each other up working building restoring together our prayers are with the families who have lost loved ones and we send them our deepest condolences through words cannot express the sadness one feels at times like this i hope everyone in louisiana knows that our country is praying for them and standing with them to help them in these difficult hours they are very very difficult thank you we are one country one people and we will have together one great fantastic future together id like to talk about the new american future that we are going to create as a team together last week i laid out my plan to bring jobs back to our country they are vanishing and they are vanishing quickly on monday i laid out my plan to defeat radical islamic terrorism on tuesday in wisconsin i talked about how were going to restore law and order to this country we need \n",
      "\n",
      "Hclinton:\n",
      "opportunity we need to protect our national security and we have got to work toward american unity so i have been trying to understand what it is that has driven people to support trump and ive met with some people i have listened to them and so many of them are looking for an explanation as to why they lost the job they had for 18 years when the factory closed and nobody cared about them what theyre going to do when their whole life was spent mining coal and they made 80 thousand a year now they can barely find a job making minimum wage why the centers of so many old industrial towns in america are hollowed out and people are turning to opiates and heroin and the list goes on and thats what ive heard so i think we have to recognize that of course some of the appeal is xenophobic and racist and misogynistic and offensive we have to acknowledge that but lets not lose sight of the real pain that many americans are feeling because the economy has left them behind so i have said i said it again in my acceptance speech last thursday i want to be the president for all americans i want to lift up and give everybody a chance to pursue their dreams and that means people who are supporting him when i went to west virginia i knew that i was not gonna win west virginia i can tell \n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_counts = {f'{selections[1].title()}_Correct':[], \n",
    "               f'{selections[1].title()}_Incorrect':[], \n",
    "               f'{selections[0].title()}_Correct':[], \n",
    "               f'{selections[0].title()}_Incorrect':[]}\n",
    "\n",
    "for i in preds_df.index:\n",
    "    if preds_df.iloc[i]['Actual'] == 0:\n",
    "        if preds_df.iloc[i]['Accuracy'] == True:\n",
    "            word_counts[f'{selections[1].title()}_Correct'] += preds_df.iloc[i]['bigrams']\n",
    "        else:\n",
    "            word_counts[f'{selections[1].title()}_Incorrect'] += preds_df.iloc[i]['bigrams']\n",
    "            print(f'{selections[1].title()}:')\n",
    "            print(' '.join(preds_df.iloc[i]['text']), '\\n')\n",
    "    else:\n",
    "        if preds_df.iloc[i]['Accuracy'] == True:\n",
    "            word_counts[f'{selections[0].title()}_Correct'] += preds_df.iloc[i]['bigrams']\n",
    "        else:\n",
    "            word_counts[f'{selections[0].title()}_Incorrect'] += preds_df.iloc[i]['bigrams']\n",
    "            print(f'{selections[0].title()}:')\n",
    "            print(' '.join(preds_df.iloc[i]['text']), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# from operator import itemgetter\n",
    "\n",
    "# # keys = list(word_counts.keys())\n",
    "# for key in word_counts.keys():\n",
    "#     top_100 = sorted(Counter(word_counts[key]).items(), key=itemgetter(1), reverse=True)[:100]\n",
    "#     top_100_df = pd.DataFrame(top_100, columns=['word', 'count'])\n",
    "#     top_100_df.to_csv(f'{key}_top_100_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trump': 24, 'hclinton': 2}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = {}\n",
    "\n",
    "for key in word_index.keys():\n",
    "    word_counts[key] = {selections[0]: 0, selections[1]: 0}\n",
    "    \n",
    "for i in range(preds_df.shape[0]):\n",
    "    for word in preds_df.iloc[i]['bigrams']:\n",
    "        selection = selections[1] if preds_df.iloc[i]['Actual'] == 0 else selections[0]\n",
    "        word_counts[word][selection]+=1\n",
    "        \n",
    "word_counts['wall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hclinton      5976\n",
       "trump         7154\n",
       "difference    4764\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_df = pd.DataFrame.from_dict(word_counts).T\n",
    "word_counts_df['difference'] = np.abs(word_counts_df[selections[0]] - word_counts_df[selections[1]])\n",
    "word_cloud_csv = word_counts_df.sort_values(by='difference', ascending=False)[:100]\n",
    "word_cloud_csv.to_csv(f'{selections[0]}_{selections[1]}_word_cloud_data.csv')\n",
    "word_cloud_csv.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hclinton</th>\n",
       "      <th>trump</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>going</th>\n",
       "      <td>304</td>\n",
       "      <td>617</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theyre</th>\n",
       "      <td>32</td>\n",
       "      <td>172</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>83</td>\n",
       "      <td>200</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>america</th>\n",
       "      <td>136</td>\n",
       "      <td>31</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>162</td>\n",
       "      <td>260</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>315</td>\n",
       "      <td>412</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>112</td>\n",
       "      <td>207</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hillary_clinton</th>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>125</td>\n",
       "      <td>42</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>98</td>\n",
       "      <td>19</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dont</th>\n",
       "      <td>94</td>\n",
       "      <td>170</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>donald_trump</th>\n",
       "      <td>99</td>\n",
       "      <td>26</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>folks</th>\n",
       "      <td>12</td>\n",
       "      <td>83</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>71</td>\n",
       "      <td>142</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ok</th>\n",
       "      <td>5</td>\n",
       "      <td>74</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vote</th>\n",
       "      <td>110</td>\n",
       "      <td>41</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>together</th>\n",
       "      <td>94</td>\n",
       "      <td>27</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>families</th>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>98</td>\n",
       "      <td>164</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>103</td>\n",
       "      <td>37</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>235</td>\n",
       "      <td>171</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>140</td>\n",
       "      <td>203</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thats</th>\n",
       "      <td>135</td>\n",
       "      <td>194</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>159</td>\n",
       "      <td>105</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>131</td>\n",
       "      <td>79</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18</td>\n",
       "      <td>69</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>americans</th>\n",
       "      <td>78</td>\n",
       "      <td>27</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need</th>\n",
       "      <td>85</td>\n",
       "      <td>35</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>election</th>\n",
       "      <td>71</td>\n",
       "      <td>22</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>13</td>\n",
       "      <td>43</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>take_care</th>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heres</th>\n",
       "      <td>40</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kids</th>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someone</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>countries</th>\n",
       "      <td>13</td>\n",
       "      <td>41</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>part</th>\n",
       "      <td>44</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>148</td>\n",
       "      <td>120</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>59</td>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senate</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unbelievable</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>going_win</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everything</th>\n",
       "      <td>47</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everybody</th>\n",
       "      <td>60</td>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big</th>\n",
       "      <td>35</td>\n",
       "      <td>61</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look</th>\n",
       "      <td>53</td>\n",
       "      <td>79</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>32</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>183</td>\n",
       "      <td>157</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remember</th>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stronger_together</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ever</th>\n",
       "      <td>30</td>\n",
       "      <td>56</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lost</th>\n",
       "      <td>13</td>\n",
       "      <td>38</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>says</th>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>things</th>\n",
       "      <td>33</td>\n",
       "      <td>58</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fix</th>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nothing</th>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   hclinton  trump  difference\n",
       "going                   304    617         313\n",
       "theyre                   32    172         140\n",
       "great                    83    200         117\n",
       "america                 136     31         105\n",
       "country                 162    260          98\n",
       "people                  315    412          97\n",
       "right                   112    207          95\n",
       "hillary_clinton           1     95          94\n",
       "work                    125     42          83\n",
       "help                     98     19          79\n",
       "dont                     94    170          76\n",
       "donald_trump             99     26          73\n",
       "folks                    12     83          71\n",
       "way                      71    142          71\n",
       "ok                        5     74          69\n",
       "vote                    110     41          69\n",
       "together                 94     27          67\n",
       "families                 75      9          66\n",
       "like                     98    164          66\n",
       "president               103     37          66\n",
       "want                    235    171          64\n",
       "said                    140    203          63\n",
       "thats                   135    194          59\n",
       "bad                       6     61          55\n",
       "us                      159    105          54\n",
       "really                  131     79          52\n",
       "mean                     18     69          51\n",
       "americans                78     27          51\n",
       "need                     85     35          50\n",
       "election                 71     22          49\n",
       "...                     ...    ...         ...\n",
       "stop                     13     43          30\n",
       "take_care                 5     34          29\n",
       "heres                    40     11          29\n",
       "media                     0     29          29\n",
       "kids                     34      5          29\n",
       "someone                  30      2          28\n",
       "countries                13     41          28\n",
       "part                     44     16          28\n",
       "problem                   2     30          28\n",
       "think                   148    120          28\n",
       "trump                    59     31          28\n",
       "senate                   30      3          27\n",
       "unbelievable              1     28          27\n",
       "going_win                 0     27          27\n",
       "everything               47     20          27\n",
       "everybody                60     33          27\n",
       "big                      35     61          26\n",
       "look                     53     79          26\n",
       "amazing                   5     31          26\n",
       "world                    32     58          26\n",
       "get                     183    157          26\n",
       "beautiful                 4     30          26\n",
       "remember                 24     50          26\n",
       "stronger_together        26      0          26\n",
       "ever                     30     56          26\n",
       "lost                     13     38          25\n",
       "says                     39     14          25\n",
       "things                   33     58          25\n",
       "fix                       5     29          24\n",
       "nothing                  12     36          24\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cloud_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
